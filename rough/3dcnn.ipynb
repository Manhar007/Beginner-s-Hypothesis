{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\hp\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=R3D_18_Weights.KINETICS400_V1`. You can also use `weights=R3D_18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 239.3228\n",
      "Epoch 2/10, Loss: 238.7548\n",
      "Epoch 3/10, Loss: 238.7197\n",
      "Epoch 4/10, Loss: 238.6769\n",
      "Epoch 5/10, Loss: 238.8189\n",
      "Epoch 6/10, Loss: 238.7846\n",
      "Epoch 7/10, Loss: 238.7117\n",
      "Epoch 8/10, Loss: 238.6237\n",
      "Epoch 9/10, Loss: 238.7968\n",
      "Epoch 10/10, Loss: 238.5833\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from torch import nn, optim\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# Custom Dataset to load videos\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, folder_path, labels_csv, transform=None):\n",
    "        self.folder_path = folder_path\n",
    "        self.labels = pd.read_csv(labels_csv)\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Convert video_summary to tuple of (x_summary, y_summary)\n",
    "        self.labels['video_summary'] = self.labels['video_summary'].apply(eval) \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_id = self.labels.iloc[idx]['video_id']\n",
    "        video_path = os.path.join(self.folder_path, f\"{video_id}.mp4\")  # Ensure the filename matches `video_id`\n",
    "        \n",
    "        # OpenCV to read the video frames\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "        \n",
    "        # Read 20 frames from the video\n",
    "        for _ in range(20):\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "            frame = cv2.resize(frame, (64, 64))  # Resize to 64x64\n",
    "            frames.append(frame)\n",
    "        \n",
    "        cap.release()\n",
    "        \n",
    "        frames = np.stack(frames)  # Stack to get shape (20, 64, 64, 3)\n",
    "        \n",
    "        # Convert frames to the proper format: (3, 20, 64, 64)\n",
    "        frames = np.transpose(frames, (3, 0, 1, 2))  # Convert to (3, 20, 64, 64)\n",
    "        \n",
    "        frames = torch.tensor(frames, dtype=torch.float32)\n",
    "        \n",
    "        # Apply transformations if any\n",
    "        if self.transform:\n",
    "            frames = self.transform(frames)\n",
    "        \n",
    "        # Extract x_summary and y_summary\n",
    "        x_summary, y_summary = self.labels.iloc[idx]['video_summary']\n",
    "        \n",
    "        return frames, torch.tensor([x_summary, y_summary], dtype=torch.float32)\n",
    "# Define transformations (for tensor)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda x: x / 255.0)  # Normalize the frames to [0, 1]\n",
    "])\n",
    "\n",
    "# Load data and split 80% for training and 20% for testing\n",
    "folder_path = 'BH25/Training_Data/Train_Videos'  # Change to your video folder path\n",
    "labels_csv = 'BH25/Training_Data/train.csv'  # Path to your CSV file\n",
    "\n",
    "dataset = VideoDataset(folder_path, labels_csv, transform=transform)\n",
    "train_dataset, test_dataset = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "# Load pretrained ResNet3D model and modify the output layer\n",
    "class ResNet3DModel(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(ResNet3DModel, self).__init__()\n",
    "        self.resnet3d = models.video.r3d_18(pretrained=True)  # Pretrained ResNet3D\n",
    "        self.resnet3d.fc = nn.Linear(self.resnet3d.fc.in_features, num_classes)  # Modify the output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet3d(x)\n",
    "\n",
    "\n",
    "# Check if CUDA (GPU) is available, otherwise use CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize the model and move it to the appropriate device\n",
    "model = ResNet3DModel(num_classes=2).to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for frames, labels in train_loader:\n",
    "        # Move frames and labels to the same device as the model (GPU or CPU)\n",
    "        frames, labels = frames.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(frames)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'resnet3d_model.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_17008\\498386082.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('resnet3d_model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 12.3982\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "# Check if CUDA is available and set the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the trained model\n",
    "model = ResNet3DModel(num_classes=2)\n",
    "model.load_state_dict(torch.load('resnet3d_model1.pth'))\n",
    "model.to(device)  # Move the model to the selected device\n",
    "model.eval()\n",
    "\n",
    "# Initialize lists to store predictions and true labels\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "# Evaluate the model\n",
    "with torch.no_grad():\n",
    "    for frames, labels in test_loader:\n",
    "        frames, labels = frames.to(device), labels.to(device)  # Move data to the selected device\n",
    "        \n",
    "        # Forward pass to get predictions\n",
    "        outputs = model(frames)\n",
    "        \n",
    "        # Collect predictions and true labels\n",
    "        all_preds.append(outputs.cpu().numpy())\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "# Convert lists to numpy arrays for evaluation\n",
    "all_preds = np.concatenate(all_preds, axis=0)\n",
    "all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "# Calculate Mean Absolute Error (MAE)\n",
    "mae_loss = mean_absolute_error(all_labels, all_preds)\n",
    "print(f\"Mean Absolute Error (MAE): {mae_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_17008\\2215059731.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('resnet3d_model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video summaries have been saved to video_summary_results.csv\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from torchvision import transforms\n",
    "\n",
    "# Check if CUDA is available and set the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the trained model\n",
    "model = ResNet3DModel(num_classes=2)\n",
    "model.load_state_dict(torch.load('resnet3d_model.pth'))\n",
    "model.to(device)  # Move the model to the selected device\n",
    "model.eval()\n",
    "\n",
    "# Define transformation for input video frames\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((64, 64)),  # Resize if needed\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Directory containing test videos\n",
    "test_video_dir = 'BH25/Testing_Data'  # The folder where test videos are stored\n",
    "output_csv_path = 'video_summary_results.csv'  # Path to output CSV file\n",
    "\n",
    "# List to hold video_id and video_summary (tuple of x_summary, y_summary)\n",
    "video_data = []\n",
    "\n",
    "# Iterate over all videos in the test folder\n",
    "for video_id in os.listdir(test_video_dir):\n",
    "    if video_id.endswith(\".mp4\"):  # Ensure the file is a video\n",
    "        video_path = os.path.join(test_video_dir, video_id)\n",
    "        \n",
    "        # Open the video file using OpenCV\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "        \n",
    "        # Read 20 frames (or all available frames if the video has fewer than 20)\n",
    "        for _ in range(20):\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break  # Stop if video ends or fewer frames are available\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "            frames.append(transform(frame))  # Apply transform\n",
    "        \n",
    "        # If there are less than 20 frames, pad with zeros (optional, depending on your needs)\n",
    "        if len(frames) < 20:\n",
    "            for _ in range(20 - len(frames)):\n",
    "                frames.append(torch.zeros((3, 64, 64)))  # Add empty frames\n",
    "        \n",
    "        # Stack frames into a tensor of shape (20, 3, 64, 64)\n",
    "        frames_tensor = torch.stack(frames)  # Shape: [20, 3, 64, 64]\n",
    "        \n",
    "        # Permute the dimensions to match the expected input shape for 3D CNNs\n",
    "        # New shape: [3, 20, 64, 64]\n",
    "        frames_tensor = frames_tensor.permute(1, 0, 2, 3)\n",
    "\n",
    "        # Add batch dimension to the tensor (shape becomes [1, 3, 20, 64, 64])\n",
    "        video_tensor = frames_tensor.unsqueeze(0).to(device)  # Shape: [1, 3, 20, 64, 64]\n",
    "        \n",
    "        # Predict using the trained model\n",
    "        with torch.no_grad():\n",
    "            outputs = model(video_tensor)\n",
    "        \n",
    "        # Get x_summary and y_summary values from the model output\n",
    "        x_summary, y_summary = outputs.cpu().numpy().flatten()\n",
    "\n",
    "        # Add video_id and video_summary as a tuple (x_summary, y_summary) to the results list\n",
    "        video_summary = (x_summary, y_summary)\n",
    "        video_data.append([video_id.split('.')[0], video_summary])\n",
    "\n",
    "        # Release the video capture object\n",
    "        cap.release()\n",
    "\n",
    "# Create a DataFrame to save the results\n",
    "df = pd.DataFrame(video_data, columns=[\"video_id\", \"video_summary\"])\n",
    "\n",
    "# Save to CSV file\n",
    "df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(f\"Video summaries have been saved to {output_csv_path}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
